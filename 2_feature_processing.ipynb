{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from io import StringIO\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.custom_transformers\n",
    "from src.custom_transformers import (\n",
    "    DFNanThresholdColumnDropper,\n",
    "    DFColumnDropper,\n",
    "    DFColumnFilterList,\n",
    "    DFColumnMapper,\n",
    "    DFNonUniqueValColDropper,\n",
    "    DFDropNaN,\n",
    "    DFOneHotEncoder,\n",
    "    DFPctNumeric,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.set_option(\"display.width\", 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs are defined below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "raw_data_path = \"data/raw/lending_club_loans.csv\"\n",
    "cloud_storage = \"no\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cloud_storage == \"yes\":\n",
    "    az_storage_container_name = \"myconedesx7\"\n",
    "    conn_str = (\n",
    "        \"DefaultEndpointsProtocol=https;\"\n",
    "        f\"AccountName={os.getenv('AZURE_STORAGE_ACCOUNT')};\"\n",
    "        f\"AccountKey={os.getenv('AZURE_STORAGE_KEY')};\"\n",
    "        f\"EndpointSuffix={os.getenv('ENDPOINT_SUFFIX')}\"\n",
    "    )\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(conn_str=conn_str)\n",
    "\n",
    "    blobstrings = {}\n",
    "    for blob_name in [\"blobedesz38\"]:\n",
    "        blob_client = blob_service_client.get_blob_client(\n",
    "            container=az_storage_container_name, blob=blob_name\n",
    "        )\n",
    "        blobstring = blob_client.download_blob().content_as_text()\n",
    "    loans_2007 = pd.read_csv(StringIO(blobstring), skiprows=1, low_memory=False)\n",
    "else:\n",
    "    loans_2007 = pd.read_csv(raw_data_path, skiprows=1, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set aside 33% as test data\n",
    "loans_2007, _ = train_test_split(loans_2007, test_size=0.33, random_state=4321)\n",
    "loans_2007 = loans_2007.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature transformation pipeline from the first notebook will be applied here to transform the training split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_threshold = 0.5\n",
    "non_useful_cols = [\"url\", \"desc\"]\n",
    "datetime_cols = [\"issue_d\", \"last_pymnt_d\"]\n",
    "cols_one_eighteen = [\n",
    "    \"id\",\n",
    "    \"member_id\",\n",
    "    \"funded_amnt\",\n",
    "    \"funded_amnt_inv\",\n",
    "    \"grade\",\n",
    "    \"sub_grade\",\n",
    "    \"emp_title\",\n",
    "]\n",
    "cols_eighteen_thirtysix = [\n",
    "    \"zip_code\",\n",
    "    \"out_prncp\",\n",
    "    \"out_prncp_inv\",\n",
    "    \"total_pymnt\",\n",
    "    \"total_pymnt_inv\",\n",
    "    \"total_rec_prncp\",\n",
    "]\n",
    "cols_thirtyseven_end = [\n",
    "    \"total_rec_int\",\n",
    "    \"total_rec_late_fee\",\n",
    "    \"recoveries\",\n",
    "    \"collection_recovery_fee\",\n",
    "    \"last_pymnt_amnt\",\n",
    "]\n",
    "loan_status = [\"Fully Paid\", \"Charged Off\"]\n",
    "mapping_dictionary = {\"loan_status\": {\"Fully Paid\": 1, \"Charged Off\": 0}}\n",
    "four_or_less_value_columns = [\"pymnt_plan\"]\n",
    "pipe_part1 = Pipeline(\n",
    "    [\n",
    "        (\"nan\", DFNanThresholdColumnDropper(nan_threshold)),\n",
    "        (\"nouse\", DFColumnDropper(non_useful_cols)),\n",
    "        (\"dtime\", DFColumnDropper(datetime_cols)),\n",
    "        (\"c1\", DFColumnDropper(cols_one_eighteen)),\n",
    "        (\"c2\", DFColumnDropper(cols_eighteen_thirtysix)),\n",
    "        (\"c3\", DFColumnDropper(cols_thirtyseven_end)),\n",
    "        (\n",
    "            \"mapstatus\",\n",
    "            DFColumnFilterList(\"loan_status\", loan_status),\n",
    "        ),\n",
    "        (\"colmap\", DFColumnMapper(mapping_dictionary)),\n",
    "        (\"onevals\", DFNonUniqueValColDropper(1)),\n",
    "        (\"fourvals\", DFColumnDropper(four_or_less_value_columns)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_loans = pipe_part1.fit_transform(loans_2007)\n",
    "print(filtered_loans.shape)\n",
    "filtered_loans.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformed training data will now be used for further data processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll show a count of missing data by column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_counts = (\n",
    "    filtered_loans.isnull().sum().to_frame().sort_values(by=[0], ascending=False)\n",
    ")\n",
    "null_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll show the **fraction** of unique values for the first four columns above, since these are reported to contain the largest number of missing values. The fraction allows us to see the missing rows (a percentage from 0-100) in each of these columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\"pub_rec_bankruptcies\", \"emp_length\", \"revol_util\", \"title\"]:\n",
    "    display(filtered_loans[col].value_counts(normalize=True, dropna=False).to_frame())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pub_rec_bankruptcies` has both of the following\n",
    "- missing in more than 1% of the data\n",
    "- nearly 94% of its rows assigned to a single value\n",
    "\n",
    "So we'll drop this column entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_than_one_pct_missing_columns = [\"pub_rec_bankruptcies\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_loans = filtered_loans.drop(columns=more_than_one_pct_missing_columns, axis=1)\n",
    "filtered_loans = filtered_loans.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe = Pipeline(\n",
    "#     [\n",
    "#         (\"90pctnan\", DFColumnDropper(more_than_one_pct_missing_columns)),\n",
    "#         (\"nan\", DFDropNaN()),\n",
    "#     ]\n",
    "# )\n",
    "# filtered_loans = pipe.fit_transform(filtered_loans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_loans.shape)\n",
    "display(filtered_loans.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Categorical Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now explore the non-numeric columns in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_loans.dtypes.value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_columns_df = filtered_loans.select_dtypes(include=[\"object\"])\n",
    "display(object_columns_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in object_columns_df:\n",
    "    print(col + \": \" + str(object_columns_df[col].nunique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate unique values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first show the unique values in columns containing categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in list(object_columns_df):\n",
    "    display(object_columns_df[name].value_counts().to_frame())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations about `object` dtype features**\n",
    "- `addr_state` has many unique values and will create nearly 50 one-hot encoded variables (one per state in the US)\n",
    "  - for now, we'll drop it and add it back if required\n",
    "- `purpose` and `title` have overlapping information but values in `title` are repeated so we'll drop it\n",
    "- columns `home_ownership`, `verification_status`, `emp_length`, and `term` columns contain a small number of discrete categorical values, so we'll keep them and one-hot encode them\n",
    "  - `emp_length` will be treated as a numerical colum since the unique values have a natural ordering to them.i.e. 8 > 2 and 2 > 1.\n",
    "  - it seems like the duration of employment `emp_length` and whether the borrower owns a home `home_ownership` should be important in predicting the level of risk associated with approving a loan to that borrower, so these could be useful to a model looking to make such a prediction. This is further reason to keep these columns.\n",
    "- `datetime` attribute columns `earliest_cr_line` (month-year in which borrower opened their earliest reported credit line) and `last_credit_pull_d` (month-year in which Lending Club pulled credit for corresponding loan) will be dropped due to lookahead bias\n",
    "- `int_rate` and `revol_util` are actually numeric features but contain a `%` sign which makes them appear as `object` dtype so we'll strip out the `%` in order to convert them to a numerical datatype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop `datetime` and high cardinality columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll drop the `datetime`-dtype and high cardinality columns (`addr_state` US state in which the loan borrower resides), as identified above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_cols = [\"last_credit_pull_d\", \"earliest_cr_line\"]\n",
    "high_cardinality_cols = [\"addr_state\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe = Pipeline(\n",
    "#     [\n",
    "#         (\"hcardcols\", DFColumnDropper(high_cardinality_cols)),\n",
    "#         (\"dtime\", DFColumnDropper(datetime_cols)),\n",
    "#     ]\n",
    "# )\n",
    "# filtered_loans = pipe.fit_transform(filtered_loans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_loans = filtered_loans.drop(datetime_cols + high_cardinality_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_loans.shape)\n",
    "display(filtered_loans.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean by removing text from numerical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll map the employment length column from text describing the duration of employment to numerical values. We'll assume the following here\n",
    "- fewer than one year of employment will be considered `0`\n",
    "- ten years of employment or greater will be considered `10`\n",
    "  - the actual number, greater than `10`, is not provided in the data so we'll take this as `10`, which could mean 14 years of employment gets converted to `10` for modeling purposes\n",
    "\n",
    "Note that another strategy to process this column is to bucket the unique values based on some discrete window. eg. for a window of 3, we could group 1, 2 and 3 years of employment into the same group, 4, 5 and 6 years into the same group, and so on. However, the exact choice of this window is critical because it could render this feature useless in terms of its predictive power over the status of loan. With domain knowledge, we could make a more appropriate choice for such a window length if deemed appropriate. For now, we'll keep all the one-to-one mapping between the text version of the employment duration and its numerical representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_dict = {\n",
    "    \"emp_length\": {\n",
    "        \"10+ years\": 10,\n",
    "        \"9 years\": 9,\n",
    "        \"8 years\": 8,\n",
    "        \"7 years\": 7,\n",
    "        \"6 years\": 6,\n",
    "        \"5 years\": 5,\n",
    "        \"4 years\": 4,\n",
    "        \"3 years\": 3,\n",
    "        \"2 years\": 2,\n",
    "        \"1 year\": 1,\n",
    "        \"< 1 year\": 0,\n",
    "        \"n/a\": 0,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_loans = filtered_loans.replace(mapping_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe = Pipeline(\n",
    "#     [\n",
    "#         (\"texttonum\", DFColumnMapper(mapping_dictionary)),\n",
    "#     ]\n",
    "# )\n",
    "# filtered_loans = pipe.fit_transform(filtered_loans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_loans.shape)\n",
    "display(filtered_loans.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll one-hot encode the variables identified earlier for this type of processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nominal_columns = [\"home_ownership\", \"verification_status\", \"purpose\", \"term\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe = Pipeline(\n",
    "#     [\n",
    "#         (\"onehot\", DFOneHotEncoder(nominal_columns)),\n",
    "#     ]\n",
    "# )\n",
    "# filtered_loans = pipe.fit_transform(filtered_loans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_df = pd.get_dummies(filtered_loans[nominal_columns])\n",
    "filtered_loans = pd.concat([filtered_loans, dummy_df], axis=1)\n",
    "filtered_loans = filtered_loans.drop(columns=nominal_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_loans.shape)\n",
    "display(filtered_loans.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop columns with overlapping information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll drop the `title` (loan title given by the borrower) column and keep the `purpose` column (which contains the same information), as was indicated above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeated_data_cols = [\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe = Pipeline(\n",
    "#     [\n",
    "#         (\"repeats\", DFColumnDropper(repeated_data_cols)),\n",
    "#     ]\n",
    "# )\n",
    "# filtered_loans = pipe.fit_transform(filtered_loans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_loans = filtered_loans.drop(repeated_data_cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning special characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll remove the percentage sign from two of the `object` columns that should be treated as numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_to_numeric_cols = [\"int_rate\", \"revol_util\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe = Pipeline(\n",
    "#     [\n",
    "#         (\"pctcols\", DFPctNumeric(pct_to_numeric_cols, \"%\")),\n",
    "#     ]\n",
    "# )\n",
    "# filtered_loans = pipe.fit_transform(filtered_loans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in pct_to_numeric_cols:\n",
    "    filtered_loans[col] = (\n",
    "        filtered_loans[col].astype(str).str.rstrip(\"%\").astype(\"float\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_loans.shape)\n",
    "display(filtered_loans.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this step of processing, all columns in the data are now numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_loans.dtypes.to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Not Used) Processing columns containing a date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns containing a date have been dropped. Below, we'll first extract the year and month attribute from each of these columns, though these will not be added back to the data or used in any analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_colmns = [\n",
    "    \"issue_d\",  # dropped in phase 1 of processing\n",
    "    \"last_pymnt_d\",  # dropped in phase 1 of processing\n",
    "    \"earliest_cr_line\",  # dropped in phase 2 of processing\n",
    "    \"last_credit_pull_d\",  # dropped in phase 2 of p-processing\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Manual approach\n",
    "# for col in datetime_colmns:\n",
    "#     filtered_loans[col] = pd.to_datetime(filtered_loans[col], format='%b-%y')\n",
    "#     filtered_loans[f'{col}_month'] = filtered_loans[col].dt.month\n",
    "#     filtered_loans[f'{col}_year'] = filtered_loans[col].dt.year\n",
    "#     filtered_loans = filtered_loans.drop(columns=datetime_colmns)\n",
    "#     filtered_loans[f'{col}_month'] = pd.Categorical(filtered_loans[f\"{col}_month\"])\n",
    "#     filtered_loans[f'{col}_year'] = pd.Categorical(filtered_loans[f\"{col}_year\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DFDateTimeCols(TransformerMixin):\n",
    "    def __init__(self, cols, convert_to_categorical=False):\n",
    "        self.convert_to_categorical = convert_to_categorical\n",
    "        self.cols = cols\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # assumes X is a DataFrame\n",
    "        for col in self.cols:\n",
    "            X[col] = pd.to_datetime(X[col], format=\"%b-%y\")\n",
    "            X[f\"{col}_month\"] = X[col].dt.month\n",
    "            X[f\"{col}_year\"] = X[col].dt.year\n",
    "            X = X.drop(columns=self.cols)\n",
    "            if self.convert_to_categorical:\n",
    "                X[f\"{col}_year\"] = pd.Categorical(X[f\"{col}_year\"])\n",
    "                X[f\"{col}_month\"] = pd.Categorical(X[f\"{col}_month\"])\n",
    "        return X\n",
    "\n",
    "    def fit_transform(self, X, y=None, **kwargs):\n",
    "        self = self.fit(X, y)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other approaches to treat the extracted year and month during modeling, but we could also one-hot encode each of these features as a first pass at preparing them for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pipeline-based approach\n",
    "# pipe = Pipeline(\n",
    "#     [\n",
    "#         (\"dtime\", DFDateTimeCols(datetime_columns, True)),\n",
    "#         (\"onehot\", DFOneHotEncoder(datetime_columns)),\n",
    "#     ]\n",
    "# )\n",
    "# filtered_loans = pipe.fit_transform(filtered_loans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine all processing steps in part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cloud_storage == \"yes\":\n",
    "    az_storage_container_name = \"myconedesx7\"\n",
    "    conn_str = (\n",
    "        \"DefaultEndpointsProtocol=https;\"\n",
    "        f\"AccountName={os.getenv('AZURE_STORAGE_ACCOUNT')};\"\n",
    "        f\"AccountKey={os.getenv('AZURE_STORAGE_KEY')};\"\n",
    "        f\"EndpointSuffix={os.getenv('ENDPOINT_SUFFIX')}\"\n",
    "    )\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(conn_str=conn_str)\n",
    "\n",
    "    blobstrings = {}\n",
    "    for blob_name in [\"blobedesz38\"]:\n",
    "        blob_client = blob_service_client.get_blob_client(\n",
    "            container=az_storage_container_name, blob=blob_name\n",
    "        )\n",
    "        blobstring = blob_client.download_blob().content_as_text()\n",
    "    loans_2007 = pd.read_csv(StringIO(blobstring), skiprows=1, low_memory=False)\n",
    "else:\n",
    "    loans_2007 = pd.read_csv(raw_data_path, skiprows=1, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set aside 33% as test data\n",
    "loans_2007, _ = train_test_split(loans_2007, test_size=0.33, random_state=4321)\n",
    "loans_2007 = loans_2007.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_loans_reloaded = pipe_part1.fit_transform(loans_2007)\n",
    "print(filtered_loans_reloaded.shape)\n",
    "display(filtered_loans_reloaded.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_part2 = Pipeline(\n",
    "    [\n",
    "        (\"morethan1pctnan\", DFColumnDropper(more_than_one_pct_missing_columns)),\n",
    "        (\"nan\", DFDropNaN()),\n",
    "        (\"hcardcols\", DFColumnDropper(high_cardinality_cols)),\n",
    "        (\"dtime\", DFColumnDropper(datetime_cols)),\n",
    "        (\"texttonum\", DFColumnMapper(mapping_dict)),\n",
    "        (\"onehot\", DFOneHotEncoder(nominal_columns)),\n",
    "        (\"repeats\", DFColumnDropper(repeated_data_cols)),\n",
    "        (\"pctcols\", DFPctNumeric(pct_to_numeric_cols, \"%\")),\n",
    "        # (\"dtime\", DFDateTimeCols(datetime_columns, True)),\n",
    "        # (\"onehot\", DFOneHotEncoder(datetime_columns)),\n",
    "    ]\n",
    ")\n",
    "filtered_loans_pipe = pipe_part2.fit_transform(filtered_loans_reloaded)\n",
    "print(filtered_loans_pipe.shape)\n",
    "display(filtered_loans_pipe.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine all processing steps in parts 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_threshold = 0.5\n",
    "non_useful_cols = [\"url\", \"desc\"]\n",
    "datetime_cols1 = [\"issue_d\", \"last_pymnt_d\"]\n",
    "cols_one_eighteen = [\n",
    "    \"id\",\n",
    "    \"member_id\",\n",
    "    \"funded_amnt\",\n",
    "    \"funded_amnt_inv\",\n",
    "    \"grade\",\n",
    "    \"sub_grade\",\n",
    "    \"emp_title\",\n",
    "]\n",
    "cols_eighteen_thirtysix = [\n",
    "    \"zip_code\",\n",
    "    \"out_prncp\",\n",
    "    \"out_prncp_inv\",\n",
    "    \"total_pymnt\",\n",
    "    \"total_pymnt_inv\",\n",
    "    \"total_rec_prncp\",\n",
    "]\n",
    "cols_thirtyseven_end = [\n",
    "    \"total_rec_int\",\n",
    "    \"total_rec_late_fee\",\n",
    "    \"recoveries\",\n",
    "    \"collection_recovery_fee\",\n",
    "    \"last_pymnt_amnt\",\n",
    "]\n",
    "loan_status = [\"Fully Paid\", \"Charged Off\"]\n",
    "mapping_dictionary = {\"loan_status\": {\"Fully Paid\": 1, \"Charged Off\": 0}}\n",
    "four_or_less_value_columns = [\"pymnt_plan\"]\n",
    "\n",
    "more_than_one_pct_missing_columns = [\"pub_rec_bankruptcies\"]\n",
    "high_cardinality_cols = [\"addr_state\"]\n",
    "datetime_cols2 = [\"last_credit_pull_d\", \"earliest_cr_line\"]\n",
    "mapping_dict = {\n",
    "    \"emp_length\": {\n",
    "        \"10+ years\": 10,\n",
    "        \"9 years\": 9,\n",
    "        \"8 years\": 8,\n",
    "        \"7 years\": 7,\n",
    "        \"6 years\": 6,\n",
    "        \"5 years\": 5,\n",
    "        \"4 years\": 4,\n",
    "        \"3 years\": 3,\n",
    "        \"2 years\": 2,\n",
    "        \"1 year\": 1,\n",
    "        \"< 1 year\": 0,\n",
    "        \"n/a\": 0,\n",
    "    }\n",
    "}\n",
    "nominal_columns = [\"home_ownership\", \"verification_status\", \"purpose\", \"term\"]\n",
    "repeated_data_cols = [\"title\"]\n",
    "pct_to_numeric_cols = [\"int_rate\", \"revol_util\"]\n",
    "\n",
    "pipe_part_1_and_2 = Pipeline(\n",
    "    [\n",
    "        (\"nan1\", DFNanThresholdColumnDropper(nan_threshold)),\n",
    "        (\"nouse\", DFColumnDropper(non_useful_cols)),\n",
    "        (\"dtime1\", DFColumnDropper(datetime_cols1)),\n",
    "        (\"c1\", DFColumnDropper(cols_one_eighteen)),\n",
    "        (\"c2\", DFColumnDropper(cols_eighteen_thirtysix)),\n",
    "        (\"c3\", DFColumnDropper(cols_thirtyseven_end)),\n",
    "        (\n",
    "            \"mapstatus\",\n",
    "            DFColumnFilterList(\"loan_status\", loan_status),\n",
    "        ),\n",
    "        (\"colmap\", DFColumnMapper(mapping_dictionary)),\n",
    "        (\"onevals\", DFNonUniqueValColDropper(1)),\n",
    "        (\"fourvals\", DFColumnDropper(four_or_less_value_columns)),\n",
    "        (\"morethan1pctnan\", DFColumnDropper(more_than_one_pct_missing_columns)),\n",
    "        (\"nan2\", DFDropNaN()),\n",
    "        (\"hcardcols\", DFColumnDropper(high_cardinality_cols)),\n",
    "        (\"dtime2\", DFColumnDropper(datetime_cols2)),\n",
    "        (\"texttonum\", DFColumnMapper(mapping_dict)),\n",
    "        (\"onehot\", DFOneHotEncoder(nominal_columns)),\n",
    "        (\"repeats\", DFColumnDropper(repeated_data_cols)),\n",
    "        (\"pctcols\", DFPctNumeric(pct_to_numeric_cols, \"%\")),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cloud_storage == \"yes\":\n",
    "    az_storage_container_name = \"myconedesx7\"\n",
    "    conn_str = (\n",
    "        \"DefaultEndpointsProtocol=https;\"\n",
    "        f\"AccountName={os.getenv('AZURE_STORAGE_ACCOUNT')};\"\n",
    "        f\"AccountKey={os.getenv('AZURE_STORAGE_KEY')};\"\n",
    "        f\"EndpointSuffix={os.getenv('ENDPOINT_SUFFIX')}\"\n",
    "    )\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(conn_str=conn_str)\n",
    "\n",
    "    blobstrings = {}\n",
    "    for blob_name in [\"blobedesz38\"]:\n",
    "        blob_client = blob_service_client.get_blob_client(\n",
    "            container=az_storage_container_name, blob=blob_name\n",
    "        )\n",
    "        blobstring = blob_client.download_blob().content_as_text()\n",
    "    loans_2007 = pd.read_csv(StringIO(blobstring), skiprows=1, low_memory=False)\n",
    "else:\n",
    "    loans_2007 = pd.read_csv(raw_data_path, skiprows=1, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_2007, _ = train_test_split(loans_2007, test_size=0.33, random_state=4321)\n",
    "loans_2007 = loans_2007.reset_index(drop=True)\n",
    "filtered_loans_pipe_part_1_and_2 = pipe_part_1_and_2.fit_transform(loans_2007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert filtered_loans_pipe.equals(filtered_loans)\n",
    "assert filtered_loans_pipe_part_1_and_2.equals(filtered_loans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
